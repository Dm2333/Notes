
##With

    file = open("/tmp/foo.txt")
    try:
        data = file.read()
    finally:
        file.close()

以上代码, 改用with语句实现后:

    with open("/tmp/foo.txt")
     as file:
        data = file.read()

----

###如何工作

这看起来充满魔法，但不仅仅是魔法，Python对with的处理还很聪明。基本思想是with所求值的对象必须有一个__enter__()方法，一个__exit__()方法。
紧跟with后面的语句被求值后，返回对象的__enter__()方法被调用，这个方法的返回值将被赋值给as后面的变量。当with后面的代码块全部被执行完之后，将调用前面返回对象的__exit__()方法。

----

#Text Processing Beginner's Guide

##Providing information through markup

Structured text includes


****

#Urllib2

[urllib2使用总结](http://www.cnblogs.com/txw1958/archive/2012/03/12/2392067.html)
##urllib2简介
urllib2提供一个基础函数urlopen，通过向指定的URL发出请求来获取数据。最简单的形式就是

    import urllib2
    response=urllib2.urlopen('http://www.douban.com')
    html=response.read()

----

这个过程就是我们平时刷网页的代码表现形式，它基于请求-响应模型。

    response=urllib2.urlopen('http://www.douban.com')

实际上可以看作两个步骤:
我们指定一个域名并发送请求
1.

    request=urllib2.request('http://www.douban.com')

接着服务端响应来自客户端的请求
2.

    response=urllib2.urlopen(request)

----

也许你会注意到，我们平时除了刷网页的操作，还有向网页提交数据。这种提交数据的行为，urllib2会把它翻译为: 


    import urllib
    import urllib2
    url = 'http://www.douban.com'
    info = {'name' : 'Michael Foord',
              'location' : 'Northampton'}
    data = urllib.urlencode(info)  #info 需要被编码为urllib2能理解的格式，这里用到的是urllib
    req = urllib2.Request(url, data)
    response = urllib2.urlopen(req)
    the_page = response.read()

----

有的服务端有洁癖，不喜欢程序来触摸它。这个时候你需要将你的程序伪装成浏览器来发出请求。请求的方式就包含在header中。
常见的情形:

    import urllib
    import urllib2

    url = 'http://www.someserver.com/cgi-bin/register.cgi'
    user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'# 将user_agent写入头信息
    values = {'name' : 'Michael Foord',
              'location' : 'Northampton',
              'language' : 'Python' }
    headers = { 'User-Agent' : user_agent }
    data = urllib.urlencode(values)
    req = urllib2.Request(url, data, headers)
    response = urllib2.urlopen(req)
    the_page = response.read()

----

#Beautiful Soup

Beautiful Soup 是用Python写的一个HTML/XML的解析器，它可以很好的处理不规范标记并生成剖析树(parse tree)。 它提供简单又常用的导航（navigating），搜索以及修改剖析树的操作。它可以大大节省你的编程时间。

----

##搜索剖析树

1. findAll(name, attrs, recursive, text, limit, **kwargs)

    - `soup.findAll('b')`
    - `tagsStartingWithB = soup.findAll(re.compile('^b'))`
    - `soup.findAll({'title' : True, 'p' : True})`

1. find()
    - `find().findNext('table')`










































gt
